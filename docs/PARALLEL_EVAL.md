# Parallelization of Evaluations

Evaluations can be run in parallel to reduce wall-clock time when generating K candidate responses or scoring many (prompt, query, response) triples with the Critic.

---

## What is parallelized

1. **Candidate generation:** For each query, K responses (one per system prompt) are generated by the LLM. With `parallel_eval=True`, these K calls run in parallel via a thread pool (`generate_candidates_parallel`).
2. **Critic batch scoring:** When you need to score many triples (e.g. to assign fitness to the whole population on a set of queries), use `score_batch_parallel(score_fn, items, max_workers)` so each `critic.score(prompt, query, response)` runs in parallel.

---

## Configuration

- **Flow:** `HybridOptimizationFlow(..., parallel_eval=True, max_workers=4)` so that each step uses `generate_candidates_parallel` instead of `generate_candidates`.
- **Config:** `ActorCriticParams(parallel_eval=True, max_workers=4)`. When building the flow from config, pass these through so the flow uses parallel generation.
- **max_workers:** Number of threads. `None` means `min(K, 4)` for generation and `min(len(items), 4)` for batch scoring.

---

## API

### Generation

- **`generate_candidates(...)`** — sequential (one LLM call after another).
- **`generate_candidates_parallel(..., max_workers=None)`** — same signature plus `max_workers`; runs LLM calls in parallel. Order of results matches order of `system_prompts`.

### Utilities (`prompt_rl.utils.parallel`)

- **`map_parallel(fn, items, max_workers=None)`** — runs `fn(item)` for each item in parallel; returns list of results in order.
- **`run_parallel(fn, items, max_workers=None)`** — runs `fn(*item)` for each tuple in `items` in parallel; returns list of results in order.
- **`score_batch_parallel(score_fn, items, max_workers=None)`** — `items` is a list of `(system_prompt, query, response)`; runs `score_fn(p, q, r)` for each in parallel and returns list of scores in order. Example:

  ```python
  from prompt_rl.utils import score_batch_parallel

  items = [(prompt_i, query_j, response_ij) for ...]
  scores = score_batch_parallel(critic.score, items, max_workers=8)
  ```

---

## When to use

- **Parallel generation:** Use when K (number of candidates) is large or LLM latency is high; avoids K×latency sequential time.
- **Parallel Critic scoring:** Use when evaluating the whole population on many queries (e.g. fitness = mean score over a query set); run each `critic.score(...)` in parallel.
- **Thread safety:** LLM backends and Critic implementations should be thread-safe when used with these helpers (e.g. no shared mutable state without locks).
